{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f06624c8",
   "metadata": {},
   "source": [
    "# VI: First Practical Work\n",
    "\n",
    "**Authors:** Gerard Comas & Marc Franquesa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a211cc-a9d5-44d7-81bd-3882b783d0d5",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "Processing all datasets in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bbba92-799e-4490-92d2-21aa76282a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "import geopandas as gpd\n",
    "import warnings\n",
    "from shapely.geometry import shape, Point\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b586abad-1670-4e03-9a0d-2e88806064eb",
   "metadata": {},
   "source": [
    "### Collisions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7344e3-7c79-422f-ad1b-514b813ca66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions = pd.read_csv(\"./original-data/collisions.csv\")\n",
    "\n",
    "print(collisions.columns)\n",
    "print(f\"Initial amount of rows: {len(collisions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdd2b8d-7a4d-471c-8579-95ee81bbe7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a CRASH DATETIME column as well as several checks to make sure we have the correct dataset\n",
    "\n",
    "# Truncating to the hour because some (most) rows are already truncated and we don't need more information\n",
    "collisions[\"CRASH DATETIME\"] = pd.to_datetime(collisions[\"CRASH DATE\"] + \" \" + collisions[\"CRASH TIME\"]).dt.floor(\"H\")\n",
    "\n",
    "# Adding day of week column\n",
    "collisions[\"CRASH WEEKDAY\"] = collisions[\"CRASH DATETIME\"].dt.day_name()\n",
    "\n",
    "# Adding BEFORE COVID column\n",
    "collisions[\"AFTER COVID\"] = collisions['CRASH DATETIME'].dt.year == 2020\n",
    "\n",
    "print(f\"First crash: {collisions['CRASH DATETIME'].sort_values().iloc[0]}\")\n",
    "\n",
    "print(f\"Last crash of 2018: {collisions[collisions['CRASH DATETIME'].dt.year == 2018]['CRASH DATETIME'].sort_values().iloc[-1]}\")\n",
    "\n",
    "print(f\"First crash of 2020: {collisions[collisions['CRASH DATETIME'].dt.year == 2020]['CRASH DATETIME'].sort_values().iloc[0]}\")\n",
    "\n",
    "print(f\"Last crash: {collisions['CRASH DATETIME'].sort_values().iloc[-1]}\")\n",
    "\n",
    "print(f\"Collisions in 2019: {len(collisions[collisions['CRASH DATETIME'].dt.year == 2019])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14873f1a-1acf-4515-a2fe-d3917de9596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if LOCATION contains the same information as LATITUDE and LONGITUDE\n",
    "# We will take advantage of the fact that if a value is NaN in python then\n",
    "# value == value will return False\n",
    "def same_information():\n",
    "    location = collisions[\"LOCATION\"].tolist()\n",
    "    lat, lon = collisions[\"LATITUDE\"].tolist(), collisions[\"LONGITUDE\"].tolist()\n",
    "    for i, row in enumerate(location):\n",
    "        # LOCATION is not NaN\n",
    "        if row == row:\n",
    "            if not list(map(float, row[1: -1].split(\", \"))) == [lat[i], lon[i]]: return False\n",
    "        # LOCATION is NaN\n",
    "        else:\n",
    "            # If lat or lon is different to Nan return False\n",
    "            if lat[i] == lat[i] or lon[i] == lon[i]: return False\n",
    "    return True\n",
    "\n",
    "print(same_information())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c582293-d57b-4908-a9fd-8de6d749f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column selection\n",
    "cols = [\n",
    "    \"CRASH DATETIME\",\n",
    "    \"CRASH WEEKDAY\",\n",
    "    \"AFTER COVID\",\n",
    "    \"BOROUGH\",\n",
    "    \"LATITUDE\",\n",
    "    \"LONGITUDE\",\n",
    "    \"NUMBER OF PERSONS INJURED\",\n",
    "    \"NUMBER OF PERSONS KILLED\",\n",
    "    \"VEHICLE TYPE CODE 1\",\n",
    "]\n",
    "collisions = collisions[cols]\n",
    "\n",
    "# Number of missing values in each column\n",
    "print(collisions.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0815529a-4a93-4b33-ab6f-3b8c56e12fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing values with 0 for the injured/killed columns\n",
    "collisions[\"NUMBER OF PERSONS INJURED\"].fillna(0, inplace=True)\n",
    "collisions[\"NUMBER OF PERSONS KILLED\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988d9bd9-56d8-475e-9abb-e364eeaedfb3",
   "metadata": {},
   "source": [
    "We will now classify all vehicle types into these categories:\n",
    "* Bicycle\n",
    "* Car\n",
    "* E-bike\n",
    "* E-scooter\n",
    "* Truck\n",
    "* Bus\n",
    "* Motorcycle\n",
    "* Other\n",
    "* Unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30df9c6-e7d3-40d1-a2e0-7a4d83bc4869",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "classified_vehicles = {\n",
    "    \"Station Wagon/Sport Utility Vehicle\": \"Car\",\n",
    "    \"Sedan\": \"Car\",\n",
    "    \"Bus\": \"Bus\",\n",
    "    \"Tractor Truck Diesel\": \"Truck\",\n",
    "    \"Taxi\": \"Car\",\n",
    "    \"E-Scooter\": \"E-scooter\",\n",
    "    \"Flat Bed\": \"Truck\",\n",
    "    \"Motorbike\": \"Motorcycle\",\n",
    "    \"Motorcycle\": \"Motorcycle\",\n",
    "    \"Box Truck\": \"Truck\",\n",
    "    \"Pick-up Truck\": \"Truck\",\n",
    "    \"Bike\": \"Bicycle\",\n",
    "    \"Dump\": \"Truck\",\n",
    "    \"Concrete Mixer\": \"Truck\",\n",
    "    \"Van\": \"Truck\",\n",
    "    \"PK\": \"Other\",\n",
    "    \"Golf Cart\": \"Other\",\n",
    "    \"LIMO\": \"Car\",\n",
    "    \"Tanker\": \"Truck\",\n",
    "    \"AMBULANCE\": \"Ambulance\",\n",
    "    \"Convertible\": \"Car\",\n",
    "    \"E-Bike\": \"E-bike\",\n",
    "    \"Moped\": \"Motorcycle\",\n",
    "    \"Fire Truck\": \"Truck\",\n",
    "    \"nan\": \"Other\",\n",
    "    \"Tractor Truck Gasoline\": \"Truck\",\n",
    "    \"Ambulance\": \"Ambulance\",\n",
    "    \"forlift\": \"Other\",\n",
    "    \"MOTOR SKAT\": \"Other\",\n",
    "    \"FDNY LADDE\": \"Other\",\n",
    "    \"Tow Truck / Wrecker\": \"Truck\",\n",
    "    \"FIRE TRUCK\": \"Truck\",\n",
    "    \"PICK UP\": \"Other\",\n",
    "    \"Garbage or Refuse\": \"Truck\",\n",
    "    \"GARBAGE TR\": \"Truck\",\n",
    "    \"Chassis Cab\": \"Truck\",\n",
    "    \"Bulk Agriculture\": \"Other\",\n",
    "    \"Can\": \"Other\",\n",
    "    \"van\": \"Truck\",\n",
    "    \"Carry All\": \"Other\",\n",
    "    \"FLATBED FR\": \"Truck\",\n",
    "    \"Open Body\": \"Other\",\n",
    "    \"4 dr sedan\": \"Car\",\n",
    "    \"Motorscooter\": \"Motorcycle\",\n",
    "    \"Minibike\": \"Motorcycle\",\n",
    "    \"Flat Rack\": \"Other\",\n",
    "    \"Armored Truck\": \"Truck\",\n",
    "    \"School Bus\": \"Bus\",\n",
    "    \"FDNY TRUCK\": \"Truck\",\n",
    "    \"truck\": \"Truck\",\n",
    "    \"UNK\": \"Unknown\",\n",
    "    \"TRAILER\": \"Other\",\n",
    "    \"FIRTRUCK\": \"Truck\",\n",
    "    \"MOPED\": \"Motorcycle\",\n",
    "    \"Lift Boom\": \"Other\",\n",
    "    \"fdny ems\": \"Other\",\n",
    "    \"AMBULACE\": \"Ambulance\",\n",
    "    \"bus\": \"Bus\",\n",
    "    \"BOX TRUCK\": \"Truck\",\n",
    "    \"Street Swe\": \"Other\",\n",
    "    \"Scooter\": \"Motorcycle\",\n",
    "    \"FDNY fire\": \"Other\",\n",
    "    \"DELIVERY\": \"Other\",\n",
    "    \"Cement Tru\": \"Truck\",\n",
    "    \"USPS/GOVT\": \"Other\",\n",
    "    \"Pedicab\": \"Other\",\n",
    "    \"TRUCK VAN\": \"Truck\",\n",
    "    \"UTILITY\": \"Other\",\n",
    "    \"Pick up tr\": \"Other\",\n",
    "    \"UNKNOWN\": \"Unknown\",\n",
    "    \"Multi-Wheeled Vehicle\": \"Other\",\n",
    "    \"SUV\": \"Car\",\n",
    "    \"utility\": \"Other\",\n",
    "    \"POWER SHOV\": \"Other\",\n",
    "    \"DELIVERY T\": \"Other\",\n",
    "    \"SWT\": \"Other\",\n",
    "    \"Trac\": \"Other\",\n",
    "    \"FDNY AMBUL\": \"Ambulance\",\n",
    "    \"AMBU\": \"Other\",\n",
    "    \"USPS\": \"Other\",\n",
    "    \"FLAT\": \"Other\",\n",
    "    \"Beverage Truck\": \"Truck\",\n",
    "    \"E-BIKE\": \"E-bike\",\n",
    "    \"3-Door\": \"Car\",\n",
    "    \"Fork Lift\": \"Other\",\n",
    "    \"Refrigerated Van\": \"Truck\",\n",
    "    \"PSD\": \"Other\",\n",
    "    \"Fire Engin\": \"Other\",\n",
    "    \"FORKLIFT\": \"Other\",\n",
    "    \"TRAC\": \"Other\",\n",
    "    \"Tow Truck\": \"Truck\",\n",
    "    \"COURIER\": \"Other\",\n",
    "    \"Courier\": \"Other\",\n",
    "    \"Leased amb\": \"Other\",\n",
    "    \"SMART CAR\": \"Car\",\n",
    "    \"message si\": \"Other\",\n",
    "    \"scooter\": \"Motorcycle\",\n",
    "    \"E-UNICYCLE\": \"E-scooter\",\n",
    "    \"Street Cle\": \"Other\",\n",
    "    \"box\": \"Other\",\n",
    "    \"F550\": \"Truck\",\n",
    "    \"DELV\": \"Other\",\n",
    "    \"SKATEBOARD\": \"Other\",\n",
    "    \"Lawnmower\": \"Other\",\n",
    "    \"almbulance\": \"Other\",\n",
    "    \"dark color\": \"Other\",\n",
    "    \"Work Van\": \"Other\",\n",
    "    \"ford van\": \"Truck\",\n",
    "    \"ambulance\": \"Ambulance\",\n",
    "    \"Fire truck\": \"Truck\",\n",
    "    \"Minicycle\": \"Motorcycle\",\n",
    "    \"PC\": \"Other\",\n",
    "    \"box truck\": \"Truck\",\n",
    "    \"FDNY ENGIN\": \"Other\",\n",
    "    \"commercial\": \"Other\",\n",
    "    \"Unknown\": \"Unknown\",\n",
    "    \"Tractor tr\": \"Truck\",\n",
    "    \"2 dr sedan\": \"Car\",\n",
    "    \"FD LADDER\": \"Other\",\n",
    "    \"abulance\": \"Other\",\n",
    "    \"FDNY Engin\": \"Other\",\n",
    "    \"OTH\": \"Other\",\n",
    "    \"Go kart\": \"Other\",\n",
    "    \"Trailer\": \"Other\",\n",
    "    \"TRUCK\": \"Truck\",\n",
    "    \"Stake or Rack\": \"Other\",\n",
    "    \"COMMERCIAL\": \"Other\",\n",
    "    \"CHEVY EXPR\": \"Other\",\n",
    "    \"SLINGSHOT\": \"Other\",\n",
    "    \"dilevery t\": \"Other\",\n",
    "    \"FDNY #226\": \"Other\",\n",
    "    \"FREIGHT FL\": \"Other\",\n",
    "    \"Fork lift\": \"Other\",\n",
    "    \"UTIL\": \"Other\",\n",
    "    \"UNKN\": \"Other\",\n",
    "    \"FDNY FIRE\": \"Other\",\n",
    "    \"ELECTRIC S\": \"Other\",\n",
    "    \"FIRETRUCK\": \"Truck\",\n",
    "    \"MOVING VAN\": \"Truck\",\n",
    "    \"usps\": \"Other\",\n",
    "    \"moped\": \"Motorcycle\",\n",
    "    \"forklift\": \"Other\",\n",
    "    \"UPS TRUCK\": \"Truck\",\n",
    "    \"backhoe\": \"Other\",\n",
    "    \"Delv\": \"Other\",\n",
    "    \"dump truck\": \"Truck\",\n",
    "    \"Freight\": \"Other\",\n",
    "    \"Horse\": \"Other\",\n",
    "    \"Cargo Van\": \"Truck\",\n",
    "    \"USPS VAN\": \"Other\",\n",
    "    \"TRUCK FLAT\": \"Truck\",\n",
    "    \"BOBCAT FOR\": \"Other\",\n",
    "    \"Tractor Tr\": \"Truck\",\n",
    "    \"Pumper\": \"Other\",\n",
    "    \"DELIVERY V\": \"Other\",\n",
    "    \"DOT EQUIPM\": \"Other\",\n",
    "    \"fire truck\": \"Truck\",\n",
    "    \"Livestock Rack\": \"Other\",\n",
    "    \"GEN  AMBUL\": \"Ambulance\",\n",
    "    \"J1\": \"Other\",\n",
    "    \"DUMP\": \"Other\",\n",
    "    \"18 WHEELER\": \"Truck\",\n",
    "    \"MAIL TRUCK\": \"Other\",\n",
    "    \"UTILITY VE\": \"Other\",\n",
    "    \"MOTORSCOOT\": \"Motorcycle\",\n",
    "    \"government\": \"Other\",\n",
    "    \"trailer\": \"Other\",\n",
    "    \"FIRE ENGIN\": \"Other\",\n",
    "    \"Front-Load\": \"Other\",\n",
    "    \"DRILL RIG\": \"Other\",\n",
    "    \"SCOOTER\": \"Motorcycle\",\n",
    "    \"Wh Ford co\": \"Other\",\n",
    "    \"suburban\": \"Car\",\n",
    "    \"E REVEL SC\": \"Other\",\n",
    "    \"ROAD SWEEP\": \"Other\",\n",
    "    \"LIGHT TRAI\": \"Other\",\n",
    "    \"Tractor\": \"Truck\",\n",
    "    \"UT\": \"Other\",\n",
    "    \"USPS TRUCK\": \"Other\",\n",
    "    \"cross\": \"Other\",\n",
    "    \"Van Camper\": \"Other\",\n",
    "    \"AMBULENCE\": \"Ambulance\",\n",
    "    \"FOOD TRUCK\": \"Other\",\n",
    "    \"Bucket Tru\": \"Other\",\n",
    "    \"gator\": \"Other\",\n",
    "    \"FDNY Ambul\": \"Ambulance\",\n",
    "    \"JOHN DEERE\": \"Other\",\n",
    "    \"f-250\": \"Other\",\n",
    "    \"MECHANICAL\": \"Other\",\n",
    "    \"WORK VAN\": \"Other\",\n",
    "    \"NYC FD\": \"Other\",\n",
    "    \"MTA BUS\": \"Bus\",\n",
    "    \"NYC AMBULA\": \"Ambulance\",\n",
    "    \"GOLF CART\": \"Other\",\n",
    "    \"FLATBED\": \"Truck\",\n",
    "    \"Trc\": \"Other\",\n",
    "    \"FORK LIFT\": \"Other\",\n",
    "    \"Pick up Tr\": \"Other\",\n",
    "    \"postal bus\": \"Bus\",\n",
    "    \"F150XL PIC\": \"Other\",\n",
    "    \"ambu\": \"Other\",\n",
    "    \"Pick up\": \"Other\",\n",
    "    \"CAT\": \"Other\",\n",
    "    \"ELEC. UNIC\": \"E-scooter\",\n",
    "    \"1C\": \"Other\",\n",
    "    \"SCOOT\": \"Motorcycle\",\n",
    "    \"FREIG\": \"Other\",\n",
    "    \"AMBUL\": \"Ambulance\",\n",
    "    \"VAN T\": \"Other\",\n",
    "    \"MINI\": \"Other\",\n",
    "    \"Garba\": \"Other\",\n",
    "    \"motor\": \"Other\",\n",
    "    \"Lunch Wagon\": \"Other\",\n",
    "    \"E-Bik\": \"E-bike\",\n",
    "    \"Ambul\": \"Ambulance\",\n",
    "    \"FDNY\": \"Other\",\n",
    "    \"SCHOO\": \"Other\",\n",
    "    \"Comm\": \"Other\",\n",
    "    \"Fire\": \"Other\",\n",
    "    \"Sanit\": \"Other\",\n",
    "    \"mail\": \"Other\",\n",
    "    \"RV\": \"Other\",\n",
    "    \"GARBA\": \"Other\",\n",
    "    \"ambul\": \"Ambulance\",\n",
    "    \"FIRET\": \"Other\",\n",
    "    \"FIRE\": \"Other\",\n",
    "    \"SELF\": \"Other\",\n",
    "    \"STAK\": \"Other\",\n",
    "    \"WORKH\": \"Other\",\n",
    "    \"FORKL\": \"Other\",\n",
    "    \"Tract\": \"Other\",\n",
    "    \"freig\": \"Other\",\n",
    "    \"DELIV\": \"Other\",\n",
    "    \"trail\": \"Other\",\n",
    "    \"PICKU\": \"Other\",\n",
    "    \"Dumps\": \"Other\",\n",
    "    \"forkl\": \"Other\",\n",
    "    \"fire\": \"Other\",\n",
    "    \"TRK\": \"Other\",\n",
    "    \"ELECT\": \"Other\",\n",
    "    \"2- to\": \"Other\",\n",
    "    \"BROOM\": \"Other\",\n",
    "    \"TRAIL\": \"Other\",\n",
    "    \"EBIKE\": \"E-bike\",\n",
    "    \"Trail\": \"Other\",\n",
    "    \"Glass Rack\": \"Other\",\n",
    "    \"Motorized Home\": \"Other\",\n",
    "    \"US POSTAL\": \"Other\",\n",
    "    \"TRT\": \"Other\",\n",
    "    \"BLOCK\": \"Other\",\n",
    "    \"pas\": \"Other\",\n",
    "    \"COM\": \"Other\",\n",
    "    \"CONCR\": \"Other\",\n",
    "    \"Pallet\": \"Other\",\n",
    "    \"unknown\": \"Unknown\",\n",
    "    \"CHERR\": \"Other\",\n",
    "    \"UTV\": \"Other\",\n",
    "    \"MOTOR\": \"Other\",\n",
    "    \"MTA B\": \"Bus\",\n",
    "    \"TRACT\": \"Other\",\n",
    "    \"NYC\": \"Other\",\n",
    "    \"UHAUL\": \"Other\",\n",
    "    \"scoot\": \"Motorcycle\",\n",
    "    \"FED E\": \"Other\",\n",
    "    \"COMME\": \"Other\",\n",
    "    \"TRLR\": \"Other\",\n",
    "    \"LOADE\": \"Other\",\n",
    "    \"rv\": \"Other\",\n",
    "    \"TOWER\": \"Other\",\n",
    "    \"Pick\": \"Other\",\n",
    "    \"AMB\": \"Other\",\n",
    "    \"NS AM\": \"Other\",\n",
    "    \"UNKNO\": \"Unknown\",\n",
    "    \"NEW Y\": \"Other\",\n",
    "    \"TOW T\": \"Other\",\n",
    "    \"GRAY\": \"Other\",\n",
    "    \"tract\": \"Other\",\n",
    "    \"STREE\": \"Other\",\n",
    "    \"MAIL\": \"Other\",\n",
    "    \"e-bik\": \"E-bike\",\n",
    "    \"unk\": \"Unknown\",\n",
    "    \"box t\": \"Other\",\n",
    "    \"CRANE\": \"Other\",\n",
    "    \"garba\": \"Other\",\n",
    "    \"Pickup with mounted Camper\": \"Other\",\n",
    "    \"FRONT\": \"Other\",\n",
    "    \"Sprin\": \"Other\",\n",
    "    \"delv\": \"Other\",\n",
    "    \"POWER\": \"Other\",\n",
    "    \"Box t\": \"Other\",\n",
    "    \"CAMP\": \"Other\",\n",
    "    \"Enclosed Body - Removable Enclosure\": \"Other\",\n",
    "    \"RGS\": \"Other\",\n",
    "    \"GOVER\": \"Other\",\n",
    "    \"FORK\": \"Other\",\n",
    "    \"UTILI\": \"Other\",\n",
    "    \"POSTO\": \"Other\",\n",
    "    \"firet\": \"Other\",\n",
    "    \"WORK\": \"Other\",\n",
    "    \"R/V C\": \"Other\",\n",
    "    \"sgws\": \"Other\",\n",
    "    \"Cat 9\": \"Other\",\n",
    "    \"BACKH\": \"Other\",\n",
    "    \"E-MOT\": \"E-scooter\",\n",
    "    \"MACK\": \"Other\",\n",
    "    \"SPC\": \"Other\",\n",
    "    \"fork\": \"Other\",\n",
    "    \"OMR\": \"Other\",\n",
    "    \"semi\": \"Other\",\n",
    "    \"FORK-\": \"Other\",\n",
    "    \"Wheel\": \"Other\",\n",
    "    \"Utili\": \"Other\",\n",
    "    \"E-BIK\": \"E-bike\",\n",
    "    \"fd tr\": \"Other\",\n",
    "    \"SWEEP\": \"Other\",\n",
    "    \"BOX T\": \"Other\",\n",
    "    \"CASE\": \"Other\",\n",
    "    \"FD TR\": \"Other\",\n",
    "    \"Work\": \"Other\",\n",
    "    \"LIBER\": \"Other\",\n",
    "    \"fdny\": \"Other\",\n",
    "    \"COMB\": \"Other\",\n",
    "    \"HEAVY\": \"Other\",\n",
    "    \"DUMPS\": \"Other\",\n",
    "    \"MTA b\": \"Bus\",\n",
    "    \"Hopper\": \"Other\",\n",
    "    \"R/V\": \"Other\",\n",
    "    \"FOOD\": \"Other\",\n",
    "    \"FD tr\": \"Other\",\n",
    "    \"Spc\": \"Other\",\n",
    "    \"BED T\": \"Other\",\n",
    "    \"comme\": \"Other\",\n",
    "    \"UPS T\": \"Other\",\n",
    "    \"PAS\": \"Other\",\n",
    "    \"BICYC\": \"Bicycle\",\n",
    "    \"Subn\": \"Other\",\n",
    "    \"WHEEL\": \"Other\",\n",
    "    \"Util\": \"Other\",\n",
    "    \"ACCES\": \"Other\",\n",
    "    \"e sco\": \"E-scooter\",\n",
    "    \"BOBCA\": \"Other\",\n",
    "    \"TANK\": \"Other\",\n",
    "    \"TRACK\": \"Other\",\n",
    "    \"utili\": \"Other\",\n",
    "    \"DEMA-\": \"Other\",\n",
    "    \"tow\": \"Other\",\n",
    "    \"dump\": \"Other\",\n",
    "    \"Elect\": \"Other\",\n",
    "    \"deliv\": \"Other\",\n",
    "    \"Backh\": \"Other\",\n",
    "    \"CEMEN\": \"Other\",\n",
    "    \"99999\": \"Other\",\n",
    "    \"BULLD\": \"Other\",\n",
    "    \"seagr\": \"Other\",\n",
    "    \"schoo\": \"Other\",\n",
    "    \"CONST\": \"Other\",\n",
    "    \"self\": \"Other\",\n",
    "    \"BK\": \"Other\",\n",
    "    \"Semi\": \"Other\",\n",
    "    \"Scoot\": \"Motorcycle\",\n",
    "    \"NYPD\": \"Other\",\n",
    "    \"Taxis\": \"Car\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8991cdfa-5680-4a68-bae6-1696120271ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing vehicle types to classification we want to use, list is found in the \n",
    "# NYC collision dataset: ATV, bicycle, car/suv, ebike, E-scooter, truck/bus,\n",
    "# motorcycle, other, unknown\n",
    "\n",
    "collisions[\"ORIGINAL VEHICLE\"] = collisions[\"VEHICLE TYPE CODE 1\"].fillna(\"unknown\")\n",
    "collisions = collisions.drop(columns=\"VEHICLE TYPE CODE 1\")\n",
    "\n",
    "collisions[\"VEHICLE\"] = collisions[\"ORIGINAL VEHICLE\"].replace(classified_vehicles)\n",
    "\n",
    "collisions[\"VEHICLE\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d4f2f6-10d3-4066-b300-8688af84fbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing LATITUDE and longitude values that don't make make sense for NYC into NaNs\n",
    "collisions[\"LATITUDE\"] = collisions[\"LATITUDE\"].where(collisions[\"LATITUDE\"].between(38, 42))\n",
    "collisions[\"LONGITUDE\"] = collisions[\"LONGITUDE\"].where(collisions[\"LONGITUDE\"].between(-76, -72))\n",
    "\n",
    "# Adding our own LOCATION column, we do know that it already exists but it was easier for us this way\n",
    "# If either LATITUDE or LONGITUDE is NaN then location will be NaN\n",
    "def combine_columns(row):\n",
    "    if pd.notna(row[\"LATITUDE\"]) and pd.notna(row[\"LONGITUDE\"]):\n",
    "        return [row[\"LATITUDE\"], row[\"LONGITUDE\"]]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "collisions[\"LOCATION\"] = collisions.apply(combine_columns, axis=1)\n",
    "\n",
    "# Dropping NaNs in LOCATION and BOROUGH, if either BOROUGH or LOCATION is not NaN we will keep the row\n",
    "collisions.dropna(subset=[\"LOCATION\", \"BOROUGH\"], how=\"all\", inplace=True)\n",
    "\n",
    "print(f\"Current amount of rows: {len(collisions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbea8c6-1c9d-4fc5-a868-27e3b0761420",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(collisions.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbafc66b-d2de-43da-a276-5ef2cfddaf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688f4a07-0d7e-4d24-989f-f6f4d64b716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions.to_csv(\"./processed-data/collisions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7963d987-2f34-4784-8a76-3f47a1ea62db",
   "metadata": {},
   "source": [
    "### Weather dataset\n",
    "\n",
    "Data obtained from the ASOS Network of Iowa State University, with the following link: https://mesonet.agron.iastate.edu/request/download.phtml?network=NY_ASOS. <br>\n",
    "\n",
    "We have selected the station [NYC] NEW YORK CITY (1943-Now).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a2acc0-441c-459b-8d2f-12b7d05d14a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file into a pandas DataFrame\n",
    "weather_2018 = pd.read_csv(\"./original-data/NYC_weather_2018.csv\")\n",
    "weather_2020 = pd.read_csv(\"./original-data/NYC_weather_2020.csv\")\n",
    "\n",
    "# concatenate the two dataframes\n",
    "weather = pd.concat([weather_2018, weather_2020], ignore_index=True)\n",
    "\n",
    "# M & T represents a NaN in the dataset (found in the docs)\n",
    "weather = weather.replace('M', None).replace('T', None)\n",
    "\n",
    "# print the concatenated dataframe\n",
    "print(weather.columns)\n",
    "\n",
    "# na values in the dataframe\n",
    "print(weather.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following columns will be removed, as they are not deemed relevant. This may be due to the focus on summer data, where columns related to snow lack significance, or because the columns inherently possess a high number of missing values: <br>\n",
    "- station\n",
    "- dwpf\n",
    "- drct\n",
    "- alti\n",
    "- gust\n",
    "- skyc1\n",
    "- skyc2\n",
    "- skyc3\n",
    "- skyc4\n",
    "- sky1\n",
    "- skyl2\n",
    "- skyl3\n",
    "- skyl4\n",
    "- wxcodes\n",
    "- feel\n",
    "- ice_accretion_1hr\n",
    "- ice_accretion_3hr\n",
    "- ice_accretion_6hr\n",
    "- peak_wind_gust\n",
    "- peak_wind_drct\n",
    "- peak_wind_time\n",
    "- metar\n",
    "- snowdepth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"valid\",\n",
    "    \"tmpf\",\n",
    "    \"relh\",\n",
    "    \"sknt\",\n",
    "    \"p01i\",\n",
    "    \"mslp\",\n",
    "    \"vsby\",\n",
    "]\n",
    "\n",
    "weather = weather[cols]\n",
    "\n",
    "weather[cols[1:]] = weather[cols[1:]].apply(pd.to_numeric)\n",
    "\n",
    "print(weather.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The analysis will involve working with the following variables.\n",
    "- **`valid`**: timestamp of the observation\n",
    "- **`tmpf`**: Air Temperature in Fahrenheit, typically @ 2 meters \n",
    "- **`relh`**: Relative Humidity in %\n",
    "- **`sknt`**: Wind Speed in knots \n",
    "- **`p01i`**: One hour precipitation for the period from the observation time to the time of the previous hourly precipitation reset. Values are in inches.\n",
    "- **`mslp`**: Sea Level Pressure in millibar \n",
    "- **`vsby`**: Visibility in miles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visually explore the missing values using the `missingno` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "\n",
    "# matrix plot\n",
    "msno.matrix(weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `msno.matrix(weather)` generates a nullity matrix, which is a graphical representation of the absence of data in the `weather` DataFrame. Each row in the matrix corresponds to a row in the DataFrame, and white marks indicate missing values.\n",
    "\n",
    "By observing the graph, you can look for patterns in the missing data. For instance, if you notice that white marks tend to cluster in certain areas, it could suggest that the missing data is not randomly distributed but rather related to some variable or condition. As in the case of the `sknt` column, likely due to a failure in the wind speed sensor, we should take this into account when analyzing the data.\n",
    "\n",
    "We can also visually see that missing values in the `tmpf` and `relh` columns are related.\n",
    "\n",
    "Another valuable insight from this graph is that the `mslp` column may not be as interesting as initially thought, as having so many random missing values doesn't make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap\n",
    "msno.heatmap(weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `msno.heatmap(weather)` function generates a heatmap showing the correlation of missing data in the `weather` DataFrame. The values on the heatmap range from -1 to 1.\n",
    "\n",
    "A value close to 1 indicates that the presence of a missing value in one column is strongly correlated with the presence of a missing value in another column. This could suggest that missing values in both columns are being caused by the same underlying factor.\n",
    "\n",
    "On the other hand, a value close to -1 indicates that the presence of a missing value in one column is strongly correlated with the presence of a value in another column. This could suggest that missing values in one column are being caused by the absence of missing values in the other column.\n",
    "\n",
    "Now, we can confirm the strong correlation between the missing values in the `tmpf` and `relh` columns, as well as the significant correlation between these columns and `vsby`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns mslp\n",
    "weather.drop('mslp', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine the intervals where the wind speed sensor may have stopped functioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the weather dataframe to show only data from September 2018\n",
    "weather_sep2018 = weather[(weather['valid'] >= '2018-09-01') & (weather['valid'] < '2018-09-30')]\n",
    "\n",
    "# create a mask for missing values in sknt column\n",
    "mask = weather_sep2018['sknt'].isna()\n",
    "\n",
    "# create a group identifier for consecutive missing values\n",
    "group_id = (~mask).cumsum()\n",
    "\n",
    "# group the consecutive missing values together and count the number of missing values in each group\n",
    "consecutive_missing_values = mask.groupby(group_id).sum()\n",
    "\n",
    "# print the number of consecutive missing values and the last one in the series\n",
    "print(f\"Number of consecutive missing values in sknt: {consecutive_missing_values.max()}\")\n",
    "\n",
    "# obtain the first and last element with id = consecutive_missing_values.idxmax() in group_id\n",
    "group_id_max = consecutive_missing_values.idxmax()\n",
    "\n",
    "first_element_valid_sep2018 = weather_sep2018.loc[group_id.eq(group_id_max).idxmax(), 'valid']\n",
    "last_element_valid_sep2018 = weather_sep2018.loc[group_id.eq(group_id_max+1).idxmax(), 'valid']\n",
    "\n",
    "# print the valid column of the first and last element of the consecutive missing value\n",
    "print(f\"Valid column of the first element: {first_element_valid_sep2018}\")\n",
    "print(f\"Valid column of the last element: {last_element_valid_sep2018}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the weather dataframe to show only data from June 2020\n",
    "weather_jun2020 = weather[(weather['valid'] >= '2020-06-01') & (weather['valid'] < '2020-06-30')]\n",
    "\n",
    "# create a mask for missing values in sknt column\n",
    "mask = weather_jun2020['sknt'].isna()\n",
    "\n",
    "# create a group identifier for consecutive missing values\n",
    "group_id = (~mask).cumsum()\n",
    "\n",
    "# group the consecutive missing values together and count the number of missing values in each group\n",
    "consecutive_missing_values = mask.groupby(group_id).sum()\n",
    "\n",
    "# print the number of consecutive missing values and the last one in the series\n",
    "print(f\"Number of consecutive missing values in sknt: {consecutive_missing_values.max()}\")\n",
    "\n",
    "# obtain the first and last element with id = consecutive_missing_values.idxmax() in group_id\n",
    "group_id_max = consecutive_missing_values.idxmax()\n",
    "first_element_valid_jun2020 = weather_jun2020.loc[group_id.eq(group_id_max).idxmax(), 'valid']\n",
    "last_element_valid_jun2020 = weather_jun2020.loc[group_id.eq(group_id_max+1).idxmax(), 'valid']\n",
    "\n",
    "# print the valid column of the first and last element of the consecutive missing value\n",
    "print(f\"Valid column of the first element: {first_element_valid_jun2020}\")\n",
    "print(f\"Valid column of the last element: {last_element_valid_jun2020}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the identified periods of wind speed sensor malfunction, a procedure is initiated to interpolate all remaining missing values, excluding those specific intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate the missing values in sknt column except the ones between 2020-06-01 00:51 and 2020-06-19 19:51 and between 2018-09-12 14:21 and 2018-09-17 17:51\n",
    "mask1 = (weather['valid'] >= first_element_valid_jun2020) & (weather['valid'] <= last_element_valid_jun2020)\n",
    "mask2 = (weather['valid'] >= first_element_valid_sep2018) & (weather['valid'] <= last_element_valid_sep2018)\n",
    "mask = ~(mask1 | mask2)\n",
    "weather.loc[mask, 'sknt'] = weather.loc[mask, 'sknt'].interpolate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the analysis of the `p01i` column, a visualization is conducted on the values preceding missing entries. This aims to provide an approximate understanding of their magnitudes. Considering the uncertainty about sensor behavior, it is plausible that the sensor may cease recording during periods of no rainfall or excessively high rainfall amounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the rows where the value in the p01i column is not missing and the value in the shifted p01i column is missing\n",
    "before_missing = weather.loc[weather['p01i'].notna() & weather['p01i'].shift(1).isna()]\n",
    "\n",
    "ch = alt.Chart(before_missing).mark_bar().encode(\n",
    "    x='valid:O',\n",
    "    y='p01i:Q'\n",
    ")\n",
    "\n",
    "ch.properties(\n",
    "    width=800,\n",
    "    height=400\n",
    ").configure_axisX(labels=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Statistics of weather['p01i']:\\n\", weather['p01i'].describe())\n",
    "print(\"\\nStatistics of before_missing['p01i']:\\n\", before_missing['p01i'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No specific patterns or behaviors in the sensor data have been identified. Consequently, a decision has been made to interpolate the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate the missing values in p01i column\n",
    "weather['p01i'] = weather['p01i'].interpolate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned, there exists a correlation between the missing values in the `vsby` column and `tmpf`. Consequently, the decision has been made to interpolate only the values that are not correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate the missing valurs in vsby column\n",
    "weather.loc[weather['tmpf'].notna(), 'vsby'] = weather.loc[weather['tmpf'].notna(), 'vsby'].interpolate()\n",
    "\n",
    "# get the count of missing values in each column\n",
    "weather.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the conversion to the International System of Units will be performed:\n",
    "- **tmpf**: Fahrenheit to Celsius <br>\n",
    "$$ Celsius = (Fahrenheit - 32) \\cdot  \\frac{5}{9} $$ \n",
    "- **sknt**: Knots to km/h\n",
    "$$ 1 \\ knot = 1.852 \\ \\frac{km}{h}$$\n",
    "- **p01i**: inches to cm\n",
    "$$ 1 \\ inch  = 2.54 \\ cm $$\n",
    "- **vsby**: miles to km\n",
    "$$ 1 \\ mile = 1.609344 \\ km $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather[\"tmpf\"] = (weather[\"tmpf\"] - 32) * 5/9\n",
    "\n",
    "weather['sknt'] = weather['sknt'] * 1.852\n",
    "\n",
    "weather['p01i'] = weather['p01i'] * 2.54\n",
    "\n",
    "weather['vsby'] = weather['vsby'] * 1.609344\n",
    "\n",
    "\n",
    "weather['valid'] = pd.to_datetime(weather['valid']).dt.floor('H')\n",
    "weather_grouped = weather.groupby('valid').mean().reset_index()\n",
    "print(weather_grouped.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"First crash: {weather_grouped['valid'].sort_values().iloc[0]}\")\n",
    "\n",
    "print(f\"Last crash of 2018: {weather_grouped[weather_grouped['valid'].dt.year == 2018]['valid'].sort_values().iloc[-1]}\")\n",
    "\n",
    "print(f\"First crash of 2020: {weather_grouped[weather_grouped['valid'].dt.year == 2020]['valid'].sort_values().iloc[0]}\")\n",
    "\n",
    "print(f\"Last crash: {weather_grouped['valid'].sort_values().iloc[-1]}\")\n",
    "\n",
    "print(f\"Collisions in 2019: {len(weather_grouped[weather_grouped['valid'].dt.year == 2019])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.describe()\n",
    "\n",
    "weather_grouped.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataframe to a csv file\n",
    "weather_grouped.to_csv(\"processed-data/weather.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33cbfff-7e7d-4959-96de-df8dab85f1de",
   "metadata": {},
   "source": [
    "### NYC Map\n",
    "Currently using [NYC community district boundaries](https://data.cityofnewyork.us/City-Government/Community-Districts/yfnk-k7r4) in a geojson format. Lets add the number of collisions in each region. We find that community districts is the perfect granularity for a choropleth map. Not too little (Boroughs) not too much (zip codes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8e0ca5-a120-46b4-aeba-1046e4d7bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_data = gpd.read_file(f\"./original-data/map.geojson\")\n",
    "\n",
    "collisions[\"DISTRICT\"] = collisions[\"LOCATION\"].apply(\n",
    "    lambda x: [-1] if x != x else np.where(map_data.contains(Point(x[1], x[0])))[0]\n",
    ")\n",
    "\n",
    "collisions[\"DISTRICT\"] = collisions[\"DISTRICT\"].apply(lambda x: -1 if len(x) == 0 else x[0]).replace(-1, np.nan)\n",
    "\n",
    "map_data[\"collision_count\"] = collisions.groupby([\"DISTRICT\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "map_data.to_file(\"processed-data/map.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03d6d20",
   "metadata": {},
   "source": [
    "## Design and implementation\n",
    "\n",
    "**Q IDEAS:**\n",
    "\n",
    "* Q1: basic barplot?\n",
    "* Q2: slope chart\n",
    "* Q3: histogram?\n",
    "* Q4: basic map plot\n",
    "* Q5:\n",
    "\n",
    "---\n",
    "**O IDEAS:**\n",
    "* Color for vehicle type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658aa12f-7ed0-4877-b5c6-db6a8ca28bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful functions\n",
    "\n",
    "def before_covid(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df[df[\"AFTER COVID\"] == False]\n",
    "\n",
    "def after_covid(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df[df[\"AFTER COVID\"] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c81d57-200f-44a8-ab17-dfbf7dc8c6d9",
   "metadata": {},
   "source": [
    "### 1. Are accidents more frequent during weekdays or weekends? Is there any difference between before COVID-19 and after?\n",
    "\n",
    "With an ambitious goal in mind, lets first plot the total collisions of each day of the week before COVID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f09065-124f-4ddb-8c96-91bd9b6aa0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_covid_day_count = before_covid(collisions).groupby([\"CRASH WEEKDAY\"]).size().reset_index(name=\"counts\")\n",
    "\n",
    "weekdayorder = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "\n",
    "alt.Chart(before_covid_day_count).mark_bar().encode(\n",
    "    x = alt.X(\"CRASH WEEKDAY:O\", sort=weekdayorder, axis=alt.Axis(title=\"Week Day\")),\n",
    "    y = alt.Y(\"counts:Q\", axis=alt.Axis(title=\"Collisions\"))\n",
    ").properties(\n",
    "    width=400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf36cb6-f747-4b61-84dc-aa1b9e7094c2",
   "metadata": {},
   "source": [
    "Lets now make a grouped bar chart, separating before and after covid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803a7124-638c-4b81-8b44-e3811ff52f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_df = collisions.groupby([\"CRASH WEEKDAY\", \"AFTER COVID\"]).size().reset_index(name=\"counts\")\n",
    "\n",
    "before, after, all_time = \"Summer 2018 (Before Covid)\", \"Summer 2020 (After Covid)\", \"All\"\n",
    "\n",
    "days_df[\"MOMENT\"] = np.where(days_df[\"AFTER COVID\"], after, before)\n",
    "\n",
    "weekdayorder = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "\n",
    "opacity = 0.5\n",
    "\n",
    "colors = {\n",
    "    before: \"#fdc086\", # Before COVID\n",
    "    after: \"#7fc97f\", # After COVID\n",
    "    all_time: \"#beaed4\"\n",
    "}\n",
    "\n",
    "days_ch = alt.Chart(days_df).mark_bar(\n",
    "    opacity=opacity\n",
    ").encode(\n",
    "   x=alt.X(\"CRASH WEEKDAY:O\", axis=alt.Axis(labelAngle=-30, title=None), sort=weekdayorder),\n",
    "   xOffset=\"MOMENT:O\",\n",
    "   y=alt.Y(\"counts:Q\", axis=alt.Axis(title=\"Collisions\", grid=True)),\n",
    "   color=alt.Color(\"MOMENT:O\", scale=alt.Scale(domain=list(colors.keys()), range=list(colors.values())), legend=alt.Legend(title=None))\n",
    ")\n",
    "\n",
    "days_ch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eec20a-7fe6-4f04-9198-428e6e6dee7d",
   "metadata": {},
   "source": [
    "Lets now add the average of before and after covid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6ef52c-bdc2-4bba-b10f-e4b6ed372b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = alt.Chart(days_df).mark_rule(opacity=1).encode(\n",
    "    y=\"mean(counts):Q\",\n",
    "    size=alt.value(2),\n",
    "    color=\"MOMENT:O\"\n",
    ")\n",
    "\n",
    "averages + days_ch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544f66b6-a4e5-4538-8aac-96c391db20bd",
   "metadata": {},
   "source": [
    "Lets now separate the days of the week in two categories, weekdays and weekends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c80974a-6de2-4f37-996f-abe6e8b4a57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekdays = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"]\n",
    "weekends = [\"Saturday\", \"Sunday\"]\n",
    "\n",
    "weekdays_df = days_df[days_df[\"CRASH WEEKDAY\"].isin(weekdays)]\n",
    "weekends_df = days_df[days_df[\"CRASH WEEKDAY\"].isin(weekends)]\n",
    "\n",
    "weekdays_ch = alt.Chart(weekdays_df).mark_bar(opacity=opacity).encode(\n",
    "   x=alt.X(\"CRASH WEEKDAY:O\", axis=alt.Axis(labelAngle=-30, title=None), sort=weekdayorder),\n",
    "   xOffset=\"MOMENT:O\",\n",
    "   y=alt.Y(\"counts:Q\", axis=alt.Axis(title=\"Collisions / Means\", grid=True), scale=alt.Scale(domain=[0, 13000])),\n",
    "   color=alt.Color(\"MOMENT:O\", scale=alt.Scale(domain=list(colors.keys()), range=list(colors.values())))\n",
    ").properties(title=alt.Title(\"Weekdays\", fontSize=10, fontWeight=600))\n",
    "\n",
    "averages_weekday = alt.Chart(weekdays_df).mark_rule(opacity=1).encode(\n",
    "    y=\"mean(counts):Q\",\n",
    "    size=alt.value(2),\n",
    "    color=alt.Color(\"MOMENT:O\")\n",
    ")\n",
    "\n",
    "\n",
    "weekends_ch = alt.Chart(weekends_df).mark_bar(opacity=opacity).encode(\n",
    "   x=alt.X(\"CRASH WEEKDAY:O\", axis=alt.Axis(labelAngle=-30, title=None), sort=weekdayorder),\n",
    "   xOffset=\"MOMENT:O\",\n",
    "   y=alt.Y(\n",
    "       \"counts:Q\",\n",
    "       axis=alt.Axis(title=None, labels=False, domain=False, ticks=False, grid=True),\n",
    "       scale=alt.Scale(domain=[0, 13000])\n",
    "   ),\n",
    "   color=alt.Color(\n",
    "       \"MOMENT:O\",\n",
    "       scale=alt.Scale(domain=list(colors.keys()), range=list(colors.values())),\n",
    "       legend=alt.Legend(title=None)\n",
    "   )\n",
    ").properties(title=alt.Title(\"Weekends\", fontSize=10, fontWeight=600))\n",
    "\n",
    "averages_weekend = alt.Chart(weekends_df).mark_rule(opacity=1).encode(\n",
    "    y=\"mean(counts):Q\",\n",
    "    size=alt.value(2),\n",
    "    color=\"MOMENT:O\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "q1 = ((weekdays_ch + averages_weekday) | (weekends_ch + averages_weekend))\n",
    "\n",
    "q1.configure_legend(symbolOpacity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac0cf70-159a-4d17-b902-7fef0d9db59a",
   "metadata": {},
   "source": [
    "### 2. Is there any type of vehicle more prone to participate in accidents?\n",
    "Obviously, with the current data we have this is impossible, as cars are the most predominant vehicle by a large margin, meaning they will have the most collisions. Lets start off viewing this data with a simle bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886101f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles = collisions.groupby([\"VEHICLE\"]).size().reset_index(name=\"counts\")\n",
    "\n",
    "alt.Chart(vehicles).mark_bar().encode(\n",
    "    y=alt.Y(\"counts:Q\", axis=alt.Axis(title=\"Collisions\")),\n",
    "    x=alt.X(\"VEHICLE:O\", axis=alt.Axis(title=None, labelAngle=-30))\n",
    ").properties(\n",
    "    width=400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0df887c",
   "metadata": {},
   "source": [
    "This confirms what we hypothesized earlier.\n",
    "\n",
    "La primera idea que vam tenir va ser la de fer un paralel coordinate plane, on tinguessim els seguents plans:\n",
    "- Percentatge d'accidents\n",
    "- Percentatge de circulació\n",
    "- Percentatge de ferits\n",
    "- Percentatge de morts\n",
    "- Ratio de ferits/accident\n",
    "- Ratio de ferits/mort\n",
    "\n",
    "Però a les dades proporcioandes no disposem del percentatge de circulació de cada vehicle i buscant per internet no hem trobat cap dataset que ens pugui proporcionar aquesta informació. Ara mirarem com es distribueixen els seguents plans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles = collisions[[\"VEHICLE\",\"NUMBER OF PERSONS INJURED\", \"NUMBER OF PERSONS KILLED\"]]\n",
    "vehicles = vehicles[vehicles[\"VEHICLE\"] != \"Unknown\"]\n",
    "\n",
    "vehicles = vehicles.groupby(\"VEHICLE\").agg({\n",
    "    \"VEHICLE\": \"count\",\n",
    "    \"NUMBER OF PERSONS INJURED\": \"sum\",\n",
    "    \"NUMBER OF PERSONS KILLED\": \"sum\"\n",
    "}).rename(columns={\"VEHICLE\": \"COLLISIONS\"}).reset_index()\n",
    "\n",
    "total_collisions = vehicles[\"COLLISIONS\"].sum()\n",
    "\n",
    "# Calcular el número de accidentes por tipo de vehículo\n",
    "vehicles[\"% COLLISIONS\"] = vehicles[\"COLLISIONS\"] / total_collisions * 100\n",
    "\n",
    "# Calcular el número total de personas heridas y muertas en todos los accidentes\n",
    "total_injured = vehicles[\"NUMBER OF PERSONS INJURED\"].sum()\n",
    "total_killed = vehicles[\"NUMBER OF PERSONS KILLED\"].sum()\n",
    "\n",
    "# Calcular los porcentajes de personas heridas y muertas para cada tipo de vehículo\n",
    "vehicles[\"% INJURED\"] = vehicles[\"NUMBER OF PERSONS INJURED\"] / total_injured * 100\n",
    "vehicles[\"% KILLED\"] = vehicles[\"NUMBER OF PERSONS KILLED\"] / total_killed * 100\n",
    "\n",
    "# Calcular los ratios de personas heridas y muertas por accidente para cada tipo de vehículo\n",
    "vehicles[\"INJURED PER COLLISION\"] = vehicles[\"NUMBER OF PERSONS INJURED\"] / vehicles[\"COLLISIONS\"]\n",
    "vehicles[\"KILLED PER COLLISION\"] = vehicles[\"NUMBER OF PERSONS KILLED\"] / vehicles[\"COLLISIONS\"]\n",
    "\n",
    "vehicles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = alt.Chart(vehicles, width=800).transform_window(\n",
    "    index=\"count()\"\n",
    ").transform_fold(\n",
    "    [\"% COLLISIONS\", \"INJURED PER COLLISION\", \"KILLED PER COLLISION\"]\n",
    ").transform_joinaggregate(\n",
    "    min=\"min(value)\",\n",
    "    max=\"max(value)\",\n",
    "    groupby=[\"key\"]\n",
    ").transform_calculate(\n",
    "    norm_val=\"(datum.value - datum.min) / (datum.max - datum.min)\",\n",
    "    mid=\"(datum.min + datum.max) / 2\"\n",
    ")\n",
    "\n",
    "lines = base.mark_line(opacity=0.3).encode(\n",
    "    x=\"key:N\",\n",
    "    y= alt.Y(\"norm_val:Q\", axis=None),\n",
    "    color=\"VEHICLE:N\",\n",
    "    detail=\"index:N\",\n",
    "    opacity=alt.value(0.5)\n",
    ")\n",
    "\n",
    "rules = base.mark_rule(\n",
    "    color=\"#ccc\", tooltip=None\n",
    ").encode(\n",
    "    x=\"key:N\",\n",
    "    detail=\"count():Q\",\n",
    ") \n",
    "\n",
    "def ytick(yvalue, field):\n",
    "    scale = base.encode(x=\"key:N\", y=alt.value(yvalue), text=f\"min({field}):Q\")\n",
    "    return alt.layer(\n",
    "        scale.mark_text(baseline=\"middle\", align=\"right\", dx=-5, tooltip=None),\n",
    "        scale.mark_tick(size=8, color=\"#ccc\", orient=\"horizontal\", tooltip=None)\n",
    "    )\n",
    "\n",
    "alt.layer(\n",
    "    lines, rules ,ytick(0, \"max\"), ytick(150, \"mid\"), ytick(300, \"min\")\n",
    ").configure_axisX(\n",
    "    domain=False, labelAngle=0, tickColor=\"#ccc\", title=None\n",
    ").configure_view(\n",
    "    stroke=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now try a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum = max(vehicles[\"COLLISIONS\"])\n",
    "minimum = min(vehicles[\"COLLISIONS\"])\n",
    "mean = vehicles[\"COLLISIONS\"].mean()\n",
    "\n",
    "# Using purple color as it represents the entire collision count\n",
    "scatter = alt.Chart(vehicles).mark_circle(color=colors[all_time]).encode(\n",
    "    x=alt.X(\"INJURED PER COLLISION:Q\", axis=alt.Axis(title=\"Injured per collision\")),\n",
    "    y=alt.Y(\"KILLED PER COLLISION:Q\", axis=alt.Axis(title=\"Deaths per collision\")),\n",
    "    size=alt.Size(\"COLLISIONS:Q\", scale=alt.Scale(range=[10, 700]), legend=alt.Legend(title=\"Total collisions (min-mean-max)\", values=[minimum, mean, maximum])),\n",
    ").properties(\n",
    "    width=500,\n",
    "    height=300\n",
    ")\n",
    "\n",
    "# Lets add labels for each vehicle\n",
    "labels = scatter.mark_text(\n",
    "    align=\"right\",\n",
    "    dx=-15,\n",
    "    dy=0\n",
    ").encode(\n",
    "    text=\"VEHICLE:N\",\n",
    "    size=alt.value(10)\n",
    ")\n",
    "\n",
    "q2 = scatter + labels\n",
    "\n",
    "q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one seems to be easier to understand and also looks nicer, we have decided to keep this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(q1 & q2).configure_legend(symbolOpacity=1).resolve_scale(size=\"independent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccceb09a",
   "metadata": {},
   "source": [
    "### 3. At what time of the day are accidents more common?\n",
    "Lets make a simpler historgram with the overall average as well as a little mark indicating the max hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7931b9-ec27-4e67-aabd-73760d727cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df = collisions\n",
    "time_df[\"HOUR\"] = time_df[\"CRASH DATETIME\"].dt.hour\n",
    "time_df = time_df.groupby([\"HOUR\", \"AFTER COVID\"]).size().reset_index(name=\"counts\")\n",
    "\n",
    "time_df[\"MOMENT\"] = np.where(time_df[\"AFTER COVID\"], after, before)\n",
    "\n",
    "time_ch = alt.Chart(time_df).mark_bar(opacity=opacity).encode(\n",
    "    x=alt.X(\"HOUR:O\", axis=alt.Axis(labelAngle=0), title=\"Hour\"),\n",
    "    y=alt.Y(\"counts:Q\", title=\"Collisions / Mean\"),\n",
    "    color=alt.Color(\n",
    "        \"MOMENT:O\",\n",
    "        scale=alt.Scale(domain=list(colors.keys()), range=list(colors.values())),\n",
    "        legend=alt.Legend(title=None)\n",
    "    ),\n",
    "    order=alt.Order(\"MOMENT:O\", sort=\"ascending\")\n",
    ")\n",
    "\n",
    "time_all_df = time_df.groupby([\"HOUR\"]).sum().reset_index()\n",
    "\n",
    "averages_weekend = alt.Chart(time_all_df).mark_rule(opacity=1, color=colors[all_time]).encode(\n",
    "    y=\"mean(counts):Q\",\n",
    "    size=alt.value(2),\n",
    ")\n",
    "\n",
    "max_hour = alt.Chart().mark_text(text=str(sum(time_df.loc[time_df[\"HOUR\"] == 16, \"counts\"])), angle=0).encode(\n",
    "    x=alt.value(330),\n",
    "    y=alt.value(20),\n",
    ")\n",
    "\n",
    "q3 = (time_ch + averages_weekend + max_hour)\n",
    "q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d586f5-be41-4b64-8e25-f797640275c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "((q1 | q3) & q2).configure_legend(symbolOpacity=1).resolve_scale(size=\"independent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e09726",
   "metadata": {},
   "source": [
    "### 4. Are there any areas with a larger number of accidents?\n",
    "Lets make a choropleth map. First, lets just a couple collisions in NYC. We are using a district map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c033f70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = alt.Chart(map_data).mark_geoshape(fill=\"lightgray\", stroke=\"black\").project(type=\"albersUsa\").properties(\n",
    "    width=700,\n",
    "    height=700\n",
    ")\n",
    "\n",
    "pts = alt.Chart(collisions[collisions[\"LOCATION\"].notna()].head(5000)).mark_circle().encode(\n",
    "    latitude=\"LATITUDE\",\n",
    "    longitude=\"LONGITUDE\",\n",
    "    color='BOROUGH',\n",
    "    tooltip=['LATITUDE', \"LONGITUDE\"]\n",
    ")\n",
    "\n",
    "(base + pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d44e4fe",
   "metadata": {},
   "source": [
    "Now making the Choropleth Map! We will be using the purple scale as we will be using the entire dataset, not just before/after covid. Keep in mind that we will only be looking at area, there are other factors too, like total km of streets. However, we have decided to go with this path as any other variable would be tricky to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23899598",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = alt.Chart(map_data).mark_geoshape().project(type=\"albersUsa\").encode(\n",
    "    color=alt.Color(\"collision_count:Q\", scale=alt.Scale(scheme='purples'), legend=alt.Legend(title=\"Collisions\")),\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=600,\n",
    "    title=\"NYC Community Districts\"\n",
    ")\n",
    "\n",
    "base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6681777",
   "metadata": {},
   "source": [
    "Lets add labels to the top 3 areas with most collisions. Only 3 as getting too many more would overcrowd the map. Getting the labels from [here](https://furmancenter.org/files/sotc/SOC2007_IndexofCommunityDistricts_000.pdf). Using the centroids of the areas to get where to place the labels. Lets see how that looks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cd76ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "top = map_data.sort_values(by=\"collision_count\", ascending=False).head(3)\n",
    "top[[\"LATITUDE\", \"LONGITUDE\"]] = top[\"geometry\"].centroid.apply(lambda x: pd.Series([x.y, x.x]))\n",
    "\n",
    "# \n",
    "labels = {\n",
    "    \"boro_cd\": [\"412\", \"413\", \"305\"],\n",
    "    \"LABELS\": [\"Jamaica / Hollis\", \"Queens Village\", \"East New York\"]\n",
    "}\n",
    "\n",
    "top = top.merge(pd.DataFrame(labels), left_on=\"boro_cd\", right_on=\"boro_cd\")\n",
    "\n",
    "text_labels = alt.Chart(top).mark_text(angle=0, dx=0, dy=0, fill=\"white\", size=8).encode(\n",
    "    longitude='LONGITUDE:Q',\n",
    "    latitude='LATITUDE:Q',\n",
    "    text='LABELS:N',\n",
    ")\n",
    "\n",
    "base + text_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0162fc",
   "metadata": {},
   "source": [
    "Labels are good except the Queens Village, which is barely visible. Lets place it where it can be read correctly. And lets add a couple icons for \"interesting vehicles\"!. These icons will be wherever they collided!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3ac60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top.loc[top[\"LABELS\"] == \"Queens Village\", [\"LATITUDE\", \"LONGITUDE\"]] = [40.66605, -73.75998]\n",
    "\n",
    "\n",
    "text_labels = alt.Chart(top).mark_text(angle=0, dx=0, dy=0, fill=\"white\", size=9).encode(\n",
    "    longitude=\"LONGITUDE:Q\",\n",
    "    latitude=\"LATITUDE:Q\",\n",
    "    text=\"LABELS:N\",\n",
    ")\n",
    "\n",
    "\n",
    "horse = alt.Chart(collisions[collisions[\"ORIGINAL VEHICLE\"] == \"Horse\"]).mark_text(text=\"🐎\", size=18).encode(\n",
    "    longitude=\"LONGITUDE:Q\",\n",
    "    latitude=\"LATITUDE:Q\",\n",
    ")\n",
    "\n",
    "gokart = alt.Chart(collisions[collisions[\"ORIGINAL VEHICLE\"] == \"Go kart\"]).mark_text(text=\"🏎️\", size=18).encode(\n",
    "    longitude=\"LONGITUDE:Q\",\n",
    "    latitude=\"LATITUDE:Q\",\n",
    ")\n",
    "\n",
    "\n",
    "q4 = (base + text_labels + horse + gokart).properties(width=600, height=600)\n",
    "\n",
    "q4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020303f6",
   "metadata": {},
   "source": [
    "Great! Lets now put it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aff4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "((q4 | (q1 & q3)) & q2).configure_legend(symbolOpacity=1).resolve_scale(size=\"independent\", color=\"shared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.  Is there a correlation between weather conditions and accidents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read weather data\n",
    "weather = pd.read_csv(\"./processed-data/weather.csv\")\n",
    "\n",
    "weather_corr = weather.drop(columns=[\"valid\"]).corr()\n",
    "weather_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the data into a long format\n",
    "corr_long = weather_corr.stack().reset_index()\n",
    "corr_long.columns = ['x', 'y', 'value']\n",
    "\n",
    "# create the heatmap\n",
    "heatmap = alt.Chart(corr_long).mark_rect().encode(\n",
    "    x='x:O',\n",
    "    y='y:O',\n",
    "    color='value:Q'\n",
    ").properties(\n",
    "    width=300,\n",
    "    height=300\n",
    ")\n",
    "\n",
    "# add text to the heatmap\n",
    "text = heatmap.mark_text(baseline='middle').encode(\n",
    "    text=alt.Text('value:Q', format='.2f'),\n",
    "    color=alt.condition(\n",
    "        alt.datum.value > 0.5,\n",
    "        alt.value('white'),\n",
    "        alt.value('black')\n",
    "    )\n",
    ")\n",
    "\n",
    "heatmap + text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir d'aquest heatmap podem veure que hi ha una gran relació entre les columnes `vsby` i `relh`, els valors de visibilitat baixa están relacionats amb alts valors d'humitat relativa. També hi ha una gran relació entre les columnes `relh` i `tmpf`. Tot això, si ens parem a pensar amb els aspectes físics té molt sentit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the collision dataset\n",
    "collisions['DATE'] = pd.to_datetime(collisions['CRASH DATETIME'])\n",
    "weather['DATE'] = pd.to_datetime(weather['valid'])\n",
    "\n",
    "\n",
    "# merge the two datasets on the common column \"DATE\"\n",
    "collisions_weather = pd.merge(collisions, weather, on=\"DATE\")\n",
    "\n",
    "# print the merged dataset\n",
    "print(collisions_weather.columns)\n",
    "\n",
    "# select the columns we want to keep\n",
    "collisions_weather_selected  = collisions_weather[['DATE', 'NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED', 'VEHICLE',  'tmpf', 'relh', 'sknt', 'p01i', 'vsby']]\n",
    "\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def violinPlot(dataset, column, rang):\n",
    "    color = '#7fc97fbb' if dataset.equals(weather) else '#beaed4'\n",
    "    title = 'Normal' if dataset.equals(weather) else 'Collisions'\n",
    "    orient = 'right' if dataset.equals(weather) else 'left'\n",
    "    chart = alt.Chart(dataset , width=100).transform_density(\n",
    "        column,\n",
    "        as_=[column, 'density'],\n",
    "        extent= rang\n",
    "    ).mark_area(orient='horizontal', color = color).encode(\n",
    "        alt.X('density:Q')\n",
    "            .stack('center')\n",
    "            .impute(None)\n",
    "            .title(None)\n",
    "            .axis(labels=False, values=[0], grid=False, ticks=True),\n",
    "        alt.Y(column + ':Q').title(title).axis(titleColor=color, orient=orient)\n",
    "    )\n",
    "\n",
    "    # Calculate quartiles\n",
    "    q1 = dataset[column].quantile(0.25)\n",
    "    q2 = dataset[column].quantile(0.5)\n",
    "    q3 = dataset[column].quantile(0.75)\n",
    "\n",
    "    # Add quartiles as horizontal lines\n",
    "    q1_r = alt.Chart(pd.DataFrame({'y': [q1]})).mark_rule(color='#fee0d2', strokeWidth=2).encode(y='y')\n",
    "    q2_r = alt.Chart(pd.DataFrame({'y': [q2]})).mark_rule(color='#fc9272', strokeWidth=2).encode(y='y')\n",
    "    q3_r = alt.Chart(pd.DataFrame({'y': [q3]})).mark_rule(color='#de2d26', strokeWidth=2).encode(y='y')\n",
    "\n",
    "    return chart + q1_r + q2_r + q3_r\n",
    "\n",
    "(violinPlot(collisions_weather_selected, 'tmpf', [5, 45]) | \n",
    " violinPlot(weather, 'tmpf', [5, 45])\n",
    ").properties(\n",
    "    title = \"Temperature\"\n",
    ") | (violinPlot(collisions_weather_selected, 'relh', [0, 100]) | \n",
    " violinPlot(weather, 'relh', [0, 100])\n",
    ").properties(\n",
    "    title = \"Humidity\"\n",
    ") | (violinPlot(collisions_weather_selected, 'sknt', [0, 25]) | \n",
    " violinPlot(weather, 'sknt', [0, 25])\n",
    ").properties(\n",
    "    title = \"Speed of wind\"\n",
    ") | (violinPlot(collisions_weather_selected, 'p01i', [0, 0.5]) | \n",
    " violinPlot(weather, 'p01i', [0, 0.5])\n",
    ").properties(\n",
    "    title = \"Rainfall level\"\n",
    ") | (violinPlot(collisions_weather_selected, 'vsby', [0, 20]) | \n",
    " violinPlot(weather, 'vsby', [0, 20])\n",
    ").properties(\n",
    "    title = \"Visibility\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amb aquest plot podem comparar la distribució de les variables climátiques quan hi ha accidents i aquestes mateixes en tot moment. La idea a l'hora de fer aquest gràfic era ajudar-nos a entendre si aquestes distribucions varien quan hi ha accidents, és a dir, si algunes de aquestes variables meteorologiques afecta en el nombre d'accidents.  \n",
    "En alguns casos veiem distribucions lleugerament diferents però d'aquesta forma no ho podem comparar facilment. Hem de buscar una altra forma de respondre a la pregunta. La idea serà buscar els casos més extrems. Començarem amb la visibilitat, on clarament veiem que el primer quartil es troba a un nivell més baix quan hi ha accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Visibility in accidents: {collisions_weather_selected['vsby'].describe()}\")\n",
    "print(f\"Visibility  in general: {weather['vsby'].describe()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir d'aquestes dades es pot concloure que quan la visibilitat és més baixa hi ha més accidents, doncs la mitjana així ho indica i el primer quartil també és més baix. Els altres quartils es troben en un valor de 16.093440 kilometres, que equival a 10 milles, valor considerat per la font de dades com una visibilitat complerta. \n",
    "Però caldria estudiar quina es la probabilitat d'accident amb visibilitat baixa enfront a la probabilitat d'accident amb visibilitat alta.\n",
    "\n",
    "Una forma més clara de representar-ho seria la seguent: \n",
    "- Histograma on les X sigui la visibilitat y la Y el nombre d'accidents/vegades que apareix en weather\n",
    "\n",
    "D'aquesta forma podem saber el ratio de colisions, que ens donarà una informació més valiosa sobre "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 17 bins for the vsby column\n",
    "bins = pd.cut(collisions_weather_selected.dropna(subset=[\"vsby\"])[\"vsby\"], bins=17, labels=list(range(17)))\n",
    "\n",
    "# group by the bins\n",
    "grouped = collisions_weather_selected.groupby(bins)\n",
    "\n",
    "# get the count of collisions in each bin\n",
    "counts = grouped.size()\n",
    "\n",
    "\n",
    "# create 17 bins for the vsby column\n",
    "bins_weather = pd.cut(weather.dropna(subset=[\"vsby\"])['vsby'], bins=17, labels=range(17))\n",
    "\n",
    "# group by the bins\n",
    "grouped_weather = weather.groupby(bins_weather)\n",
    "\n",
    "# get the count of collisions in each bin\n",
    "counts_weather = grouped_weather.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with counts and counts_weather\n",
    "df = pd.DataFrame({'counts': counts, 'counts_weather': counts_weather})\n",
    "\n",
    "# create a new column with the ratio of counts to counts_weather\n",
    "df['ratio'] = df['counts'] / df['counts_weather']\n",
    "\n",
    "df['visibility'] = df.index \n",
    "\n",
    "# create the bar chart\n",
    "chart = alt.Chart(df).mark_bar().encode(\n",
    "    x=alt.X('visibility:O', axis=alt.Axis(title='Visibility')),\n",
    "    y=alt.Y('ratio:Q', axis=alt.Axis(title='Ratio of Collisions')),\n",
    ").properties(\n",
    "    width=400,\n",
    "    height=300\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# add the mean to the plot\n",
    "mean_line = alt.Chart(df).mark_rule(color='red', strokeDash=[5,5]).encode(\n",
    "    y='mean(ratio):Q'\n",
    ")\n",
    "\n",
    "chart + mean_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean and standard deviation of the ratio\n",
    "mean_ratio = df['ratio'].mean()\n",
    "std_ratio = df['ratio'].std()\n",
    "\n",
    "# calculate the Z-Score for each data point\n",
    "df['z_score'] = (df['ratio'] - mean_ratio) / std_ratio\n",
    "\n",
    "# create the scatter plot\n",
    "scatter = alt.Chart(df).mark_circle().encode(\n",
    "    x=alt.X('visibility:O', axis=alt.Axis(title='Visibility')),\n",
    "    y=alt.Y('z_score:Q', axis=alt.Axis(title='Z-Score of Ratio')),\n",
    "    color=alt.Color('z_score:Q', scale=alt.Scale(scheme='purplegreen')),\n",
    "    tooltip=['visibility', 'z_score']\n",
    ").properties(\n",
    "    width=400,\n",
    "    height=300\n",
    ")\n",
    "\n",
    "# add the mean line to the plot\n",
    "mean_line = alt.Chart(df).mark_rule(color='red', strokeDash=[5,5]).encode(\n",
    "    y='mean(z_score):Q'\n",
    ")\n",
    "\n",
    "scatter + mean_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El que podem fer es agrupar els dos gràfics i tenim les des següents solucions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with counts and counts_weather\n",
    "df = pd.DataFrame({'counts': counts, 'counts_weather': counts_weather})\n",
    "\n",
    "# create a new column with the ratio of counts to counts_weather\n",
    "df['ratio'] = df['counts'] / df['counts_weather']\n",
    "\n",
    "df['visibility'] = df.index \n",
    "\n",
    "mean_ratio = df['ratio'].mean()\n",
    "std_ratio = df['ratio'].std()\n",
    "\n",
    "df['pstd'] = mean_ratio + 2*std_ratio\n",
    "df['nstd'] = mean_ratio - 2*std_ratio\n",
    "\n",
    "# calculate the Z-Score for each data point\n",
    "df['z_score'] = (df['ratio'] - mean_ratio) / std_ratio\n",
    "\n",
    "# create the bar chart\n",
    "bar = alt.Chart(df).mark_bar().encode(\n",
    "    x=alt.X('visibility:O', axis=alt.Axis(title='Visibility')),\n",
    "    y=alt.Y('ratio:Q', axis=alt.Axis(title='Ratio of Collisions')),\n",
    "    color=alt.Color('z_score:Q', scale=alt.Scale(scheme='purplegreen'), legend = alt.Legend(title='Z-Score of Ratio'))\n",
    ").properties(\n",
    "    width=400,\n",
    "    height=300\n",
    ")\n",
    "\n",
    "# create the bar chart\n",
    "rule = alt.Chart(df).mark_rule().encode(\n",
    "    x=alt.X('visibility:O', axis=alt.Axis(title='Visibility')),\n",
    "    y=alt.Y('ratio:Q', axis=alt.Axis(title='Ratio of Collisions')),\n",
    "    color=alt.Color('z_score:Q', scale=alt.Scale(scheme='purplegreen'), legend = None)\n",
    ").properties(\n",
    "    width=400,\n",
    "    height=300\n",
    ")\n",
    "\n",
    "# create the bar chart\n",
    "point = alt.Chart(df).mark_circle().encode(\n",
    "    x=alt.X('visibility:O', axis=alt.Axis(title='Visibility')),\n",
    "    y=alt.Y('ratio:Q', axis=alt.Axis(title='Ratio of Collisions')),\n",
    "    color=alt.Color('z_score:O', scale=alt.Scale(scheme='purplegreen'), legend= None)\n",
    ").properties(\n",
    "    width=400,\n",
    "    height=300\n",
    ")\n",
    "\n",
    "# add the mean to the plot\n",
    "mean_line = alt.Chart(df).mark_rule(color='gray', strokeDash=[5,5]).encode(\n",
    "    y='mean(ratio):Q'\n",
    ")\n",
    "\n",
    "pstd_line = alt.Chart(df).mark_rule(color='black', strokeDash=[5,5]).encode(\n",
    "    y='pstd:Q'\n",
    ")\n",
    "\n",
    "nstd_line = alt.Chart(df).mark_rule(color='black', strokeDash=[5,5]).encode(\n",
    "    y='nstd:Q'\n",
    ")\n",
    "\n",
    "(bar + mean_line + pstd_line + nstd_line) | (rule + point + mean_line + pstd_line + nstd_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cap dels punts te un Z-Score superior a 2 per tant no podem extreure conclusions molt rellevants, però encara així podem veure que quan hi ha visibilitat practicament nula, ens trobem al voltant de 1.5 desviacions estandars i per visibilitat total ens trobem per sota de una desviació estandar. \n",
    "\n",
    "Perque els resultats son menys concluyents dels esperats? doncs perque la visibilitat està molt relacionada amb la humitat i aquesta amb la temperatura, que a la vegada està molt relacionada amb l'hora del dia. Per tant podem imaginar que les hores que hi ha menys visibilittat, coincideixen amb les hores en les quals hi ha menys accidents. Cosa que comprobarem seguidament. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the hour from the DATE column\n",
    "collisions_weather_selected['HOUR'] = collisions_weather_selected['DATE'].dt.hour\n",
    "\n",
    "# group by hour and calculate the mean of the visibility column\n",
    "mean_visibility = collisions_weather_selected.groupby('HOUR')['vsby'].mean()\n",
    "\n",
    "\n",
    "# create a chart with the mean visibility by hour\n",
    "visby_hour = alt.Chart(mean_visibility.reset_index()).mark_bar().encode(\n",
    "    x=alt.X('HOUR:O', axis=alt.Axis(title='Hour')),\n",
    "    y=alt.Y('vsby:Q', axis=alt.Axis(title='Mean Visibility')),\n",
    ").properties(\n",
    "    width=400,\n",
    "    height=300\n",
    ")\n",
    "\n",
    "# add the mean line\n",
    "mean_line = alt.Chart(mean_visibility.reset_index()).mark_rule(color='red', strokeDash=[5,5]).encode(\n",
    "    y='mean(vsby):Q'\n",
    ")\n",
    "\n",
    "visby_hour + mean_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podem veure que entre les 6 i les 7 son les hores on menys visibilitat hi ha, que coincideixen amb hores amb poques colisions. Això no es una relació de causalitat, es a dir, no perque hi hagi menys visibilitat hi ha menys accidents, això no tindria sentit, sinó que just les hores on hi ha menys visibilitat, son en les que menys gent condueix i per tant menys accidents hi ha. Ens falten dades, però probablement sí que hi hauria més posibilitat d'accidents en aquestes hores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column with the visibility category\n",
    "collisions_weather_selected['VISIBILITY CATEGORY'] = np.where(collisions_weather_selected['vsby'] > 16, 'High Visibility', 'Low Visibility')\n",
    "\n",
    "# group by hour and visibility category and calculate the count of collisions\n",
    "hourly_visibility = collisions_weather_selected.groupby(['HOUR', 'VISIBILITY CATEGORY']).size().reset_index(name='counts')\n",
    "\n",
    "\n",
    "# calculate the total number of collisions per hour\n",
    "hourly_total = hourly_visibility.groupby('HOUR')['counts'].sum().reset_index(name='total')\n",
    "\n",
    "# merge the hourly_visibility and hourly_total dataframes\n",
    "hourly_visibility = pd.merge(hourly_visibility, hourly_total, on='HOUR')\n",
    "\n",
    "# calculate the percentage of low and high visibility collisions\n",
    "hourly_visibility['percentage'] = hourly_visibility['counts'] / hourly_visibility['total'] * 100\n",
    "\n",
    "# create the stacked bar chart\n",
    "stacked_bar = alt.Chart(hourly_visibility).mark_bar().encode(\n",
    "    x=alt.X('HOUR:O', axis=alt.Axis(title='Hour')),\n",
    "    y=alt.Y('percentage:Q', axis=alt.Axis(title='Percentage of Collisions')),\n",
    "    color=alt.Color('VISIBILITY CATEGORY:N', scale=alt.Scale(domain=['Low Visibility', 'High Visibility'], range=['#1f77b4', '#ff7f0e']), legend=alt.Legend(title='Visibility Category'))\n",
    ").properties(\n",
    "    width=400,\n",
    "    height=300\n",
    ")\n",
    "\n",
    "stacked_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now do the same with rainfall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# select the rows where p01i is 0\n",
    "zero_p01i = collisions_weather_selected.loc[collisions_weather_selected['p01i'] == 0]\n",
    "\n",
    "# get the number of rows with p01i = 0\n",
    "num_zero_p01i = len(zero_p01i)\n",
    "\n",
    "# select the rows where p01i is not 0\n",
    "nonzero_p01i = collisions_weather_selected.loc[collisions_weather_selected['p01i'] != 0]\n",
    "\n",
    "# create 10 bins for the p01i column\n",
    "bins = pd.cut(nonzero_p01i['p01i'], bins=10)\n",
    "\n",
    "# get the midpoint of each interval\n",
    "midpoints = bins.apply(lambda x: x.mid.round(2))\n",
    "\n",
    "# group by the midpoints\n",
    "grouped = nonzero_p01i.groupby(midpoints)\n",
    "\n",
    "# convert the result of the groupby to a dataframe\n",
    "grouped_df = grouped.size().reset_index(name='counts')\n",
    "\n",
    "# create a new dataframe with the count of rows with p01i = 0\n",
    "zero_row = pd.DataFrame({'p01i': [0], 'counts': [num_zero_p01i]})\n",
    "\n",
    "counts = pd.merge(zero_row , grouped_df, on=['p01i', 'counts'], how=\"outer\", indicator=False)\n",
    "\n",
    "# select the rows where p01i is 0\n",
    "zero_p01i_weather = weather.loc[weather['p01i'] == 0]\n",
    "\n",
    "# get the number of rows with p01i = 0\n",
    "num_zero_p01i_weather = len(zero_p01i_weather)\n",
    "\n",
    "# select the rows where p01i is not 0\n",
    "nonzero_p01i_weather = weather.loc[weather['p01i'] != 0]\n",
    "\n",
    "# create 10 bins for the p01i column\n",
    "bins_weather = pd.cut(nonzero_p01i_weather['p01i'], bins=10)\n",
    "\n",
    "# get the midpoint of each interval\n",
    "midpoints_weather = bins_weather.apply(lambda x: x.mid.round(2))\n",
    "\n",
    "# group by the midpoints\n",
    "grouped_weather = nonzero_p01i_weather.groupby(midpoints_weather)\n",
    "\n",
    "# convert the result of the groupby to a dataframe\n",
    "grouped_df_weather = grouped_weather.size().reset_index(name='counts')\n",
    "\n",
    "# create a new dataframe with the count of rows with p01i = 0\n",
    "zero_row_weather = pd.DataFrame({'p01i': [0], 'counts': [num_zero_p01i_weather]})\n",
    "\n",
    "counts_weather= pd.merge(zero_row_weather, grouped_df_weather, on=['p01i', 'counts'], how=\"outer\", indicator=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with counts and counts_weather\n",
    "df = pd.DataFrame({'p01i': counts['p01i'] ,'counts': counts['counts'], 'counts_weather': counts_weather['counts']})\n",
    "\n",
    "# create a new column with the ratio of counts to counts_weather\n",
    "df['ratio'] = df['counts'] / df['counts_weather']\n",
    "\n",
    "mean_ratio = df['ratio'].mean()\n",
    "std_ratio = df['ratio'].std()\n",
    "\n",
    "df['mean'] = mean_ratio\n",
    "df['pstd'] = mean_ratio + 2*std_ratio\n",
    "df['nstd'] = mean_ratio - 2*std_ratio\n",
    "\n",
    "# calculate the Z-Score for each data point\n",
    "df['z_score'] = (df['ratio'] - mean_ratio) / std_ratio\n",
    "\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# create the bar chart\n",
    "bar = alt.Chart(df).mark_bar().encode(\n",
    "    x=alt.X('p01i:O', axis=alt.Axis(title='Rain level')),\n",
    "    y=alt.Y('ratio:Q', axis=alt.Axis(title='Ratio of Collisions')),\n",
    "    color=alt.Color('z_score:Q', scale=alt.Scale(scheme='purplegreen'), legend = alt.Legend(title='Z-Score of Ratio'))\n",
    ").properties(\n",
    "    width=400,\n",
    "    height=300\n",
    ")\n",
    "\n",
    "# create the bar chart\n",
    "rule = alt.Chart(df).mark_rule().encode(\n",
    "    x=alt.X('p01i:O', axis=alt.Axis(title='Rain level')),\n",
    "    y=alt.Y('ratio:Q', axis=alt.Axis(title='Ratio of Collisions')),\n",
    "    color=alt.Color('z_score:Q', scale=alt.Scale(scheme='purplegreen'), legend = None)\n",
    ").properties(\n",
    "    width=400,\n",
    "    height=300\n",
    ")\n",
    "\n",
    "# create the bar chart\n",
    "point = alt.Chart(df).mark_circle().encode(\n",
    "    x=alt.X('p01i:O', axis=alt.Axis(title='Rain Level')),\n",
    "    y=alt.Y('ratio:Q', axis=alt.Axis(title='Ratio of Collisions')),\n",
    "    color=alt.Color('z_score:O', scale=alt.Scale(scheme='purplegreen'), legend= None)\n",
    ").properties(\n",
    "    width=400,\n",
    "    height=300\n",
    ")\n",
    "\n",
    "# add the mean to the plot\n",
    "mean_line = alt.Chart(df).mark_rule(color='gray', strokeDash=[5,5]).encode(\n",
    "    y='mean:Q'\n",
    ")\n",
    "\n",
    "pstd_line = alt.Chart(df).mark_rule(color='black', strokeDash=[5,5]).encode(\n",
    "    y='pstd:Q'\n",
    ")\n",
    "\n",
    "nstd_line = alt.Chart(df).mark_rule(color='black', strokeDash=[5,5]).encode(\n",
    "    y='nstd:Q'\n",
    ")\n",
    "\n",
    "(bar + mean_line + pstd_line + nstd_line) | (rule + point + mean_line + pstd_line + nstd_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultat que podriem esperar és que el ratio de colisions puja a mesura que aumenta el nivell de la pluja, i això es així fins a valors de 2.69 cm de pluja. Exceptuant la barra de 2.28 doncs sembla que tenim un valor atípic, doncs només ha plogut un cop en aquell interval. També es sorprenen que per a 3.93 torni a baixar, però de nou es un valor atípic que només ha passat un cop. Podriem com a solució plantejar el gràfic seguuent, plou vs no plou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the rows where p01i is 0\n",
    "zero_p01i = collisions_weather_selected.loc[collisions_weather_selected['p01i'] == 0]\n",
    "\n",
    "# get the number of rows with p01i = 0\n",
    "num_zero_p01i = len(zero_p01i)\n",
    "\n",
    "# select the rows where p01i is not 0\n",
    "nonzero_p01i = collisions_weather_selected.loc[collisions_weather_selected['p01i'] != 0]\n",
    "\n",
    "# get the number of rows with p01i = 0\n",
    "num_nonzero_p01i = len(nonzero_p01i)\n",
    "\n",
    "# create a new dataframe with the count of rows with p01i = 0\n",
    "zero_row = pd.DataFrame({'p01i': [0], 'counts': [num_zero_p01i]})\n",
    "\n",
    "non_zero_row = pd.DataFrame({'p01i': [1], 'counts': [num_nonzero_p01i]})\n",
    "\n",
    "counts = pd.merge(zero_row , non_zero_row, on=['p01i', 'counts'], how=\"outer\", indicator=False)\n",
    "\n",
    "# select the rows where p01i is 0\n",
    "zero_p01i_weather = weather.loc[weather['p01i'] == 0]\n",
    "\n",
    "# get the number of rows with p01i = 0\n",
    "num_zero_p01i_weather = len(zero_p01i_weather)\n",
    "\n",
    "# select the rows where p01i is not 0\n",
    "nonzero_p01i_weather = weather[weather['p01i'] != 0]\n",
    "\n",
    "# get the number of rows with p01i = 0\n",
    "num_nonzero_p01i_weather = len(nonzero_p01i_weather)\n",
    "\n",
    "# create a new dataframe with the count of rows with p01i = 0\n",
    "zero_row_weather = pd.DataFrame({'p01i': [0], 'counts': [num_zero_p01i_weather]})\n",
    "\n",
    "non_zero_row_weather = pd.DataFrame({'p01i': [1], 'counts': [num_nonzero_p01i_weather]})\n",
    "\n",
    "counts_weather = pd.merge(zero_row_weather , non_zero_row_weather, on=['p01i', 'counts'], how=\"outer\", indicator=False)\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with counts and counts_weather\n",
    "df = pd.DataFrame({'p01i': counts['p01i'] ,'counts': counts['counts'], 'counts_weather': counts_weather['counts']})\n",
    "\n",
    "# create a new column with the ratio of counts to counts_weather\n",
    "df['ratio'] = df['counts'] / df['counts_weather']\n",
    "\n",
    "# create the bar chart\n",
    "bar = alt.Chart(df).mark_bar().encode(\n",
    "    x=alt.X('p01i:O', axis=alt.Axis(title='Rain level')),\n",
    "    y=alt.Y('ratio:Q', axis=alt.Axis(title='Ratio of Collisions')),\n",
    ").properties(\n",
    "    width=400,\n",
    "    height=300\n",
    ")\n",
    "\n",
    "bar "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
